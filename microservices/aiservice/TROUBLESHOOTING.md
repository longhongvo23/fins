# AI Service - H∆∞·ªõng d·∫´n kh·∫Øc ph·ª•c s·ª± c·ªë

## üîß C√°c v·∫•n ƒë·ªÅ ƒë√£ fix

### ‚úÖ 1. MongoDB Authentication Error
**V·∫•n ƒë·ªÅ**: `Command createIndexes requires authentication`

**Gi·∫£i ph√°p**: ƒê√£ th√™m credentials v√†o MONGODB_URI
```yaml
MONGODB_URI=mongodb://${STOCKSERVICE_MONGODB_USER}:${STOCKSERVICE_MONGODB_PASSWORD}@stockservice-mongodb:27017/stockservice?authSource=admin
```

### ‚úÖ 2. Kafka GroupCoordinatorNotAvailableError
**V·∫•n ƒë·ªÅ**: `Group Coordinator Request failed: [Error 15]`

**Nguy√™n nh√¢n**: 
- Kafka topics ch∆∞a ƒë∆∞·ª£c t·∫°o
- Consumer c·ªë g·∫Øng subscribe v√†o topic kh√¥ng t·ªìn t·∫°i

**Gi·∫£i ph√°p**:
1. M·∫∑c ƒë·ªãnh disable Kafka consumer: `KAFKA_CONSUMER_ENABLED=false`
2. T·∫°o topics th·ªß c√¥ng sau khi Kafka ready:
   ```bash
   cd microservices/docker-compose
   setup-kafka-topics.bat  # Windows
   ```
3. Enable consumer sau khi topics ƒë√£ t·∫°o (optional)

### ‚úÖ 3. aiservice kh√¥ng hi·ªÉn th·ªã trong Consul
**V·∫•n ƒë·ªÅ**: Service kh√¥ng register v·ªõi Consul

**Gi·∫£i ph√°p**: ƒê√£ th√™m Consul registration
- Auto-register khi start
- Health check endpoint
- Service metadata (tags, version, etc.)

### ‚úÖ 4. Ch∆∞a c√≥ historical data
**V·∫•n ƒë·ªÅ**: System m·ªõi kh√¥ng c√≥ data ƒë·ªÉ predict

**Gi·∫£i ph√°p**:
- Th√™m validation: minimum 30 days data required
- Tr·∫£ v·ªÅ error message r√µ r√†ng
- `/api/symbols` endpoint hi·ªÉn th·ªã data statistics
- Disable auto-prediction m·∫∑c ƒë·ªãnh: `AUTO_PREDICTION_ENABLED=false`

## üöÄ Quy tr√¨nh kh·ªüi ƒë·ªông ƒë√∫ng

### B∆∞·ªõc 1: Start containers
```bash
cd microservices/docker-compose
docker-compose up -d --build
```

### B∆∞·ªõc 2: ƒê·ª£i services ready (30-60s)
```bash
# Ki·ªÉm tra services ƒëang ch·∫°y
docker-compose ps

# Xem logs
docker-compose logs -f
```

### B∆∞·ªõc 3: T·∫°o Kafka topics
```bash
# Sau khi Kafka ƒë√£ ready (xem logs kh√¥ng c√≤n error)
setup-kafka-topics.bat  # Windows
./setup-kafka-topics.sh  # Linux/Mac
```

### B∆∞·ªõc 4: Verify Kafka topics
```bash
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092

# K·∫øt qu·∫£ mong ƒë·ª£i:
# stock.updates
# predictions.recommendations
```

### B∆∞·ªõc 5: Check Consul
```bash
# M·ªü browser
http://localhost:8500

# Ki·ªÉm tra Services tab
# aiservice s·∫Ω hi·ªÉn th·ªã v·ªõi:
# - Status: passing (green)
# - Tags: ai, prediction, prophet, python
```

### B∆∞·ªõc 6: Test aiservice
```bash
# Health check
curl http://localhost:8086/health

# K·∫øt qu·∫£ mong ƒë·ª£i:
{
  "status": "healthy",
  "service": "aiservice",
  "version": "1.0.0",
  "dependencies": {
    "mongodb": "healthy",
    "kafka": "healthy"
  }
}

# Ki·ªÉm tra symbols available
curl http://localhost:8086/api/symbols
```

### B∆∞·ªõc 7: ƒê·ª£i crawlservice thu th·∫≠p data
```bash
# Xem crawlservice logs
docker-compose logs -f crawlservice

# Ki·ªÉm tra data trong MongoDB
docker exec -it stockservice-mongodb mongosh \
  -u stockservicelong -p stockservice26012003 \
  --authenticationDatabase admin

use stockservice
db.historical_price.countDocuments()
```

### B∆∞·ªõc 8: Test prediction (sau khi c√≥ ƒë·ªß data)
```bash
# Ki·ªÉm tra symbol n√†o c√≥ ƒë·ªß data
curl http://localhost:8086/api/symbols

# Test prediction
curl -X POST http://localhost:8086/api/predict \
  -H "Content-Type: application/json" \
  -d '{"symbol": "AAPL", "forecast_days": 30}'
```

## üìä Monitoring

### Ki·ªÉm tra logs realtime
```bash
# T·∫•t c·∫£ services
docker-compose logs -f

# Ch·ªâ aiservice
docker-compose logs -f aiservice

# Filter by keyword
docker-compose logs aiservice | grep -i error
docker-compose logs aiservice | grep -i kafka
docker-compose logs aiservice | grep -i consul
```

### Ki·ªÉm tra MongoDB
```bash
# Connect to MongoDB
docker exec -it stockservice-mongodb mongosh \
  -u stockservicelong -p stockservice26012003 \
  --authenticationDatabase admin

# Check databases
show dbs

# Use stockservice
use stockservice

# Check collections
show collections

# Count documents
db.historical_price.countDocuments()
db.recommendation.countDocuments()

# Check latest recommendations
db.recommendation.find().sort({period: -1}).limit(5).pretty()

# Check data per symbol
db.historical_price.aggregate([
  { $group: { _id: "$symbol", count: { $sum: 1 } } },
  { $sort: { count: -1 } },
  { $limit: 20 }
])
```

### Ki·ªÉm tra Kafka
```bash
# List topics
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092

# Describe topic
docker exec kafka kafka-topics \
  --describe \
  --topic stock.updates \
  --bootstrap-server localhost:9092

# Check consumer groups
docker exec kafka kafka-consumer-groups \
  --list \
  --bootstrap-server localhost:9092

# Check consumer group details
docker exec kafka kafka-consumer-groups \
  --describe \
  --group aiservice-consumer \
  --bootstrap-server localhost:9092
```

## üîÑ Enable Kafka Consumer (Optional)

Sau khi ƒë√£ c√≥ Kafka topics v√† mu·ªën enable real-time processing:

### C√°ch 1: Qua docker-compose.yml
```yaml
# S·ª≠a trong docker-compose.yml
environment:
  - KAFKA_CONSUMER_ENABLED=true
```

Restart service:
```bash
docker-compose restart aiservice
```

### C√°ch 2: Environment variable
```bash
docker-compose stop aiservice
docker-compose up -d aiservice -e KAFKA_CONSUMER_ENABLED=true
```

### Ki·ªÉm tra consumer ƒë√£ ch·∫°y
```bash
# Xem logs
docker-compose logs -f aiservice

# K·∫øt qu·∫£ mong ƒë·ª£i:
# Kafka consumer is enabled, starting...
# Kafka consumer started for topic: stock.updates
```

## ‚ö†Ô∏è L∆∞u √Ω quan tr·ªçng

### 1. Data Requirements
- **Minimum**: 30 ng√†y historical data cho m·ªói symbol
- **Recommended**: 90-365 ng√†y ƒë·ªÉ c√≥ prediction ch√≠nh x√°c h∆°n
- Crawlservice c·∫ßn ch·∫°y m·ªôt th·ªùi gian ƒë·ªÉ thu th·∫≠p ƒë·ªß data

### 2. Resource Requirements
- **RAM**: Minimum 2GB cho aiservice container
- **CPU**: 2 cores recommended (Prophet training c·∫ßn CPU)
- **Disk**: T√πy thu·ªôc MongoDB data size

### 3. Performance
- First prediction cho symbol: 5-30s (train model)
- Subsequent predictions: Nhanh h∆°n n·∫øu cache
- Batch predictions: Ch·∫°y background, kh√¥ng block

### 4. Production Recommendations
- Enable auto-prediction sau khi c√≥ ƒë·ªß data
- Set up monitoring v√† alerts
- Configure proper resource limits
- Enable Kafka consumer khi h·ªá th·ªëng ƒë√£ stable
- Backup MongoDB recommendation collection

## üÜò Common Issues

### Issue: "Unable to generate prediction - Insufficient data"
```bash
# Ki·ªÉm tra data available
curl http://localhost:8086/api/symbols

# ƒê·ª£i crawlservice crawl th√™m data
# Ho·∫∑c trigger manual crawl (n·∫øu c√≥ endpoint)
```

### Issue: Container keeps restarting
```bash
# Check logs
docker-compose logs aiservice

# Common causes:
# 1. MongoDB connection failed -> Check credentials
# 2. Port conflict -> Change APP_PORT
# 3. Out of memory -> Increase Docker memory limit
```

### Issue: Prediction very slow
```bash
# Prophet training l·∫ßn ƒë·∫ßu m·∫•t th·ªùi gian
# N·∫øu qu√° ch·∫≠m:
# 1. Gi·∫£m s·ªë ng√†y train data (365 -> 180)
# 2. TƒÉng CPU allocation
# 3. Consider caching trained models (future improvement)
```

## üìù Configuration Reference

### Environment Variables
```bash
# Required
MONGODB_URI=mongodb://user:pass@host:port/db?authSource=admin
KAFKA_BOOTSTRAP_SERVERS=kafka:9092

# Optional
KAFKA_CONSUMER_ENABLED=false        # true to enable real-time processing
AUTO_PREDICTION_ENABLED=false       # true to enable daily auto-predictions
SERVICE_DISCOVERY_ENABLED=true      # false to disable Consul registration
LOG_LEVEL=INFO                      # DEBUG for more logs

# Thresholds (customize as needed)
STRONG_BUY_THRESHOLD=10.0
BUY_THRESHOLD=5.0
HOLD_THRESHOLD=2.0
SELL_THRESHOLD=-5.0
STRONG_SELL_THRESHOLD=-10.0
```

### Default Ports
- **8086**: aiservice HTTP API
- **27017**: MongoDB (internal)
- **9092**: Kafka (internal)
- **8500**: Consul UI

## ‚úÖ Verification Checklist

- [ ] All containers running: `docker-compose ps`
- [ ] MongoDB accessible with auth
- [ ] Kafka topics created
- [ ] aiservice registered in Consul
- [ ] Health endpoint returns 200: `curl http://localhost:8086/health`
- [ ] API docs accessible: `http://localhost:8086/docs`
- [ ] Crawlservice collecting data
- [ ] At least one symbol has 30+ days data
- [ ] Test prediction works for that symbol

## üéØ Next Steps

1. **Monitor data collection**: ƒê·ª£i crawlservice thu th·∫≠p ƒë·ªß data (30+ ng√†y)
2. **Test predictions**: Th·ª≠ predict v·ªõi symbols c√≥ ƒë·ªß data
3. **Enable auto-prediction**: Set `AUTO_PREDICTION_ENABLED=true` khi ready
4. **Enable Kafka consumer**: Set `KAFKA_CONSUMER_ENABLED=true` cho real-time
5. **Set up monitoring**: Grafana dashboards cho metrics
6. **Production tuning**: Optimize thresholds v√† parameters
